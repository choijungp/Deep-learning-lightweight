{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mobilenet_ffff.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"4b8fe781cd1e4b28a1e224b98325817c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ad675d712f064c9891368eb29926ef03","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_290c711f81684b30a63e084a54a26029","IPY_MODEL_79e1d4593e5a472785b98078a307013a"]}},"ad675d712f064c9891368eb29926ef03":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"290c711f81684b30a63e084a54a26029":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a8fddea4a7124d2fa477a8e697ee966d","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a4ca34b0a63d499b91ba18ba7d14133b"}},"79e1d4593e5a472785b98078a307013a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3815a6dd991946f4a78df40ebb2e911d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 170500096/? [00:20&lt;00:00, 33864561.40it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bca6b477c28f42c59abc0a76af538fda"}},"a8fddea4a7124d2fa477a8e697ee966d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a4ca34b0a63d499b91ba18ba7d14133b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3815a6dd991946f4a78df40ebb2e911d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"bca6b477c28f42c59abc0a76af538fda":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"AHa5Y08JjxQN","colab_type":"code","colab":{}},"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","from torchvision.utils import save_image\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import os\n","import glob\n","import PIL\n","from PIL import Image\n","from torch.utils import data as D\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import random\n","import torchsummary\n","import time\n","\n","__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n","           'resnet152']\n","\n","\n","model_urls = {\n","    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n","    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n","    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n","    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n","    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M2BfZ1b9kxz0","colab_type":"code","colab":{}},"source":["batch_size = 64 \n","validation_ratio = 0.1\n","random_seed = 10"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"C1fRzvPUkAQh","colab_type":"code","outputId":"7dcda19e-3ce1-41ad-f031-185b25f2be38","executionInfo":{"status":"ok","timestamp":1590338583390,"user_tz":-540,"elapsed":22989,"user":{"displayName":"dy lee","photoUrl":"https://lh3.googleusercontent.com/-v5_ou76Si4Y/AAAAAAAAAAI/AAAAAAAABKQ/9DoM4jbW_BY/s64/photo.jpg","userId":"08111595787083838412"}},"colab":{"base_uri":"https://localhost:8080/","height":121,"referenced_widgets":["4b8fe781cd1e4b28a1e224b98325817c","ad675d712f064c9891368eb29926ef03","290c711f81684b30a63e084a54a26029","79e1d4593e5a472785b98078a307013a","a8fddea4a7124d2fa477a8e697ee966d","a4ca34b0a63d499b91ba18ba7d14133b","3815a6dd991946f4a78df40ebb2e911d","bca6b477c28f42c59abc0a76af538fda"]}},"source":["#데이터셋 설정\n","transform_train = transforms.Compose([\n","        transforms.Resize(224),\n","        ### 오버피팅을 방지하기 위해 RandomCrop과 Randam HorizontalFlip같은 노이즈 추가.\n","        transforms.RandomCrop(224, padding=28), #오버피팅 막으려고 랜덤으로 잘라서 이미지 만든다,,,,(?)\n","        transforms.RandomHorizontalFlip(), # 오버피팅 막으려고 이미지 반전시켜서 만든다,,,(?)\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n","\n","#validation이나 test는 그런 것 필요 없음\n","transform_validation = transforms.Compose([\n","        transforms.Resize(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n","\n","\n","transform_test = transforms.Compose([\n","        transforms.Resize(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n","\n","\n","#데이터 세트 다운로드\n","\n","trainset = torchvision.datasets.CIFAR10(\n","    root='./data', train=True, download=True, transform=transform_train)\n","\n","validset = torchvision.datasets.CIFAR10(\n","    root='./data', train=True, download=True, transform=transform_validation)\n","\n","testset = torchvision.datasets.CIFAR10(\n","    root='./data', train=False, download=True, transform=transform_test)\n","\n","#trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n","#                                          shuffle=True, num_workers=0)\n","\n","num_train = len(trainset)\n","indices = list(range(num_train))\n","split = int(np.floor(validation_ratio * num_train))\n","\n","np.random.seed(random_seed)\n","np.random.shuffle(indices)\n","\n","train_idx, valid_idx = indices[split:], indices[:split]\n","train_sampler = SubsetRandomSampler(train_idx)\n","valid_sampler = SubsetRandomSampler(valid_idx)\n","\n","\n","# train_loader에 데이터를 로드 해오는 코드들\n","train_loader = torch.utils.data.DataLoader(\n","    trainset, batch_size=batch_size, sampler=train_sampler, num_workers=0\n",")\n","\n","valid_loader = torch.utils.data.DataLoader(\n","    validset, batch_size=batch_size, sampler=valid_sampler, num_workers=0\n",")\n","\n","test_loader = torch.utils.data.DataLoader(\n","    testset, batch_size=batch_size, shuffle=False, num_workers=0\n",")\n","\n","#10개 클래스로 구분\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","# 초기 학습률\n","initial_lr = 0.1"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4b8fe781cd1e4b28a1e224b98325817c","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CubT6EwlkdzD","colab_type":"code","colab":{}},"source":["class depthwise_conv(nn.Module):\n","    # __init__()에서 모델의 구조와 동작을 정의하는 생성자를 정의(속성값을 초기화하는 역할로, 객체가 생성될 때 자동으호 호출)\n","    def __init__(self, nin, kernel_size, padding, bias=False, stride=1):\n","        super(depthwise_conv, self).__init__() # super() 함수를 부르면 여기서 만든 클래스는 nn.Module 클래스의 속성들을 가지고 초기화\n","        #nn.conv2D 모듈 : 첫번째는 입력 채널 수, 두번째변수는 출력 채널 수 세번째는 커널 사이즈(숫자하나만 지정하면 정사각형으로 간주)\n","        self.depthwise = nn.Conv2d(nin, nin, kernel_size=kernel_size, stride=stride, padding=padding, groups=nin, bias=bias)\n","        #self.depthwise는 이제 nin 크기의 받아서 nin 크기의 출력을 하는 conv2D 함수가 됨.\n","\n","    #foward() 함수는 모델이 학습데이터를 입력받아서 forward 연산을 진행시키는 함수\n","    def forward(self, x):\n","        out = self.depthwise(x)  #self.depthwise 실행하고 반환\n","        return out"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-WKIOixwke-k","colab_type":"code","colab":{}},"source":["class dw_block(nn.Module):\n","    def __init__(self, nin, kernel_size, padding=1, bias=False, stride=1):\n","        super(dw_block, self).__init__()\n","        self.dw_block = nn.Sequential(\n","            depthwise_conv(nin, kernel_size, padding, bias, stride),\n","            #BatchNorm2d(배치 정규화): 학습률을 너무 높게 잡으면 기울기가 소실되거나 발산하는 증상을 예방하여 학습과정을 안정화하는 방법\n","            nn.BatchNorm2d(nin),\n","            ##distribution을 평균 0, 표준편차 1인 input으로 normalize시키는 방법\n","            ##Training 할 때는 batch의 평균과 분산으로 normalize 하고, Test 할 때는 계산해놓은 이동 평균(training 때 계산)으로 normalize\n","            nn.ReLU()\n","        )\n","    def forward(self, x):\n","        out = self.dw_block(x)\n","        return out"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jqkWiC2-lDdr","colab_type":"code","colab":{}},"source":["class one_by_one_block(nn.Module):\n","    def __init__(self, nin, nout, padding=0, bias=False, stride=1):\n","        super(one_by_one_block, self).__init__()\n","        self.one_by_one_block = nn.Sequential(\n","            #커널 사이즈 1x1 로 컨벌루션 진행\n","            nn.Conv2d(nin, nout, kernel_size=1, stride=stride, padding=padding, bias=bias),\n","            nn.BatchNorm2d(nout),\n","            nn.ReLU()\n","        )\n","    def forward(self, x):\n","        out = self.one_by_one_block(x)\n","        return out"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sxas-JDs53Q4","colab":{}},"source":["class MobileNet(nn.Module):\n","  \n","    def __init__(self, input_channel, num_classes=10):\n","        super(MobileNet, self).__init__()\n","        \n","        self.block1 = nn.Sequential(\n","            #nn.conv2D 모듈 : 첫번째는 입력 채널 수, 두번째변수는 출력 채널 수 세번째는 커널 사이즈(숫자하나만 지정하면 정사각형으로 간주)\n","            #BatchNorm2d(배치 정규화): 학습률을 너무 높게 잡으면 기울기가 소실되거나 발산하는 증상을 예방하여 학습과정을 안정화하는 방법\n","            #계층에 들어가는 입력을 평균과 분산으로 정규화함.\n","            nn.Conv2d(input_channel, 32, kernel_size=3, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","        )\n","        #112x112x32\n","        ###identity 저장\n","        self.block2=nn.Sequential(\n","            dw_block(32, kernel_size=3), #depthwise convolution / stride=1\n","        )\n","        ###resnet 작용하기 위해 downsampling넣기\n","        self.block3=nn.Sequential( \n","            nn.ReLU(),  \n","            one_by_one_block(32, 64), #one_by_one convolution\n","            #112x112x64\n","            dw_block(64, kernel_size=3, stride=2), \n","        )\n","        #56x56x64\n","        ###identity 저장\n","        self.block4=nn.Sequential(\n","            one_by_one_block(64, 64),\n","            dw_block(64, kernel_size=3),\n","        )\n","        ###resnet 작용하기 위해 downsampling넣기\n","        self.block5=nn.Sequential(    \n","            one_by_one_block(64, 128),\n","            #56x56x128\n","            nn.ReLU(),\n","            dw_block(128, kernel_size=3, stride=2),\n","        )\n","        #28x28x128\n","        ###identity 저장\n","        self.block6=nn.Sequential( \n","            one_by_one_block(128, 128),\n","            dw_block(128, kernel_size=3),\n","        )\n","        #resnet 작용하기 위해 downsampling넣기\n","        self.block7=nn.Sequential(\n","            one_by_one_block(128, 256),\n","            #28x28x256\n","            nn.ReLU(),\n","            dw_block(256, kernel_size=3, stride=2),\n","        )\n","        #14x14x256\n","        #identity 저장\n","        self.block8=nn.Sequential(\n","            one_by_one_block(256, 128),\n","            #14x14x128   \n","            # 5 times \n","            dw_block(128, kernel_size=3),\n","            one_by_one_block(128, 128),\n","            dw_block(128, kernel_size=3),\n","            one_by_one_block(128, 128),\n","            dw_block(128, kernel_size=3),\n","            one_by_one_block(128, 128),\n","            dw_block(128, kernel_size=3),\n","            one_by_one_block(128, 128),\n","            dw_block(128, kernel_size=3),\n","            one_by_one_block(128, 256),\n","        )\n","        #14x14x256\n","        #resnet 작용하기 위해 downsampling넣기\n","        self.block9=nn.Sequential(\n","            one_by_one_block(256, 512),\n","            nn.ReLU(),\n","            dw_block(512, kernel_size=3, stride=2),\n","        )\n","        #7x7x512\n","        #identity 저장\n","        self.block10=nn.Sequential(\n","            one_by_one_block(512, 512),\n","            #7x7x512\n","            dw_block(512, kernel_size=3, padding=4, stride=2),\n","        )\n","        #7x7x512\n","        #resnet 작용하기 위해 downsampling넣기\n","        self.block11=nn.Sequential(\n","            one_by_one_block(512, 1024),\n","        )\n","        #7x7x1024\n","        #avgPool->1x1x1024  \n","\n","        #Fully Connected layer      \n","        self.fc_v2 = nn.Conv2d(1024, num_classes, 1, 1, groups=2)\n","        \n","    def forward(self, x):\n","        x = self.block1(x)\n","        identity = x\n","        x = self.block2(x)\n","        #112x112x32\n","        x += identity\n","\n","        x = self.block3(x)\n","        identity = x\n","        x = self.block4(x)\n","        x += identity\n","\n","        x = self.block5(x)\n","        identity = x\n","        x = self.block6(x)\n","        x += identity\n","\n","        x = self.block7(x)\n","        identity = x\n","        x = self.block8(x)\n","        x += identity\n","\n","        x = self.block9(x)\n","        identity = x\n","        x = self.block10(x)\n","        x += identity\n","        x = self.block11(x)\n","        \n","        body_output = x\n","        \n","        avg_pool_output = F.adaptive_avg_pool2d(body_output, (1, 1))\n","        output = self.fc_v2(avg_pool_output)\n","        output = output.view(output.size(0), -1)\n","        \n","        return output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"S4cSlV5Q53RP","colab":{}},"source":["net = MobileNet(3, 10) #아마도 인풋채널3개(RGB), 클래스 10개 로 추정됨!"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0-wjOJCW53RT","outputId":"769500af-e308-4f35-f9ec-1e5b33ebc1a2","executionInfo":{"status":"ok","timestamp":1590338588505,"user_tz":-540,"elapsed":1894,"user":{"displayName":"dy lee","photoUrl":"https://lh3.googleusercontent.com/-v5_ou76Si4Y/AAAAAAAAAAI/AAAAAAAABKQ/9DoM4jbW_BY/s64/photo.jpg","userId":"08111595787083838412"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #GPU있고 cuda를 쓸수 있으면 쿠다를 쓰게 하고 없으면 cpu 쓰게함\n","print(device)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["cuda:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TzYiY-1w53Rf","outputId":"8d068c92-d747-42e8-9e76-ec6a1b9e540a","executionInfo":{"status":"ok","timestamp":1590338604798,"user_tz":-540,"elapsed":11806,"user":{"displayName":"dy lee","photoUrl":"https://lh3.googleusercontent.com/-v5_ou76Si4Y/AAAAAAAAAAI/AAAAAAAABKQ/9DoM4jbW_BY/s64/photo.jpg","userId":"08111595787083838412"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["net.to(device) #이용가능한 device(cpu or Gpu)에 네트워크 전송"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MobileNet(\n","  (block1): Sequential(\n","    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","  )\n","  (block2): Sequential(\n","    (0): dw_block(\n","      (dw_block): Sequential(\n","        (0): depthwise_conv(\n","          (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","        )\n","        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU()\n","      )\n","    )\n","  )\n","  (block3): Sequential(\n","    (0): ReLU()\n","    (1): one_by_one_block(\n","      (one_by_one_block): Sequential(\n","        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU()\n","      )\n","    )\n","    (2): dw_block(\n","      (dw_block): Sequential(\n","        (0): depthwise_conv(\n","          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n","        )\n","        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU()\n","      )\n","    )\n","  )\n","  (block4): Sequential(\n","    (0): one_by_one_block(\n","      (one_by_one_block): Sequential(\n","        (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU()\n","      )\n","    )\n","    (1): dw_block(\n","      (dw_block): Sequential(\n","        (0): depthwise_conv(\n","          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n","        )\n","        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU()\n","      )\n","    )\n","  )\n","  (block5): Sequential(\n","    (0): one_by_one_block(\n","      (one_by_one_block): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU()\n","      )\n","    )\n","    (1): ReLU()\n","    (2): dw_block(\n","      (dw_block): Sequential(\n","        (0): depthwise_conv(\n","          (depthwise): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n","        )\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU()\n","      )\n","    )\n","  )\n","  (block6): Sequential(\n","    (0): one_by_one_block(\n","      (one_by_one_block): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU()\n","      )\n","    )\n","    (1): dw_block(\n","      (dw_block): Sequential(\n","        (0): depthwise_conv(\n","          (depthwise): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n","        )\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU()\n","      )\n","    )\n","  )\n","  (block7): Sequential(\n","    (0): one_by_one_block(\n","      (one_by_one_block): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU()\n","      )\n","    )\n","    (1): ReLU()\n","    (2): dw_block(\n","      (dw_block): Sequential(\n","        (0): depthwise_conv(\n","          (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n","        )\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU()\n","      )\n","    )\n","  )\n","  (block8): Sequential(\n","    (0): one_by_one_block(\n","      (one_by_one_block): Sequential(\n","        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU()\n","      )\n","    )\n","    (1): dw_block(\n","      (dw_block): Sequential(\n","        (0): depthwise_conv(\n","          (depthwise): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n","        )\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU()\n","      )\n","    )\n","    (2): one_by_one_block(\n","      (one_by_one_block): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU()\n","      )\n","    )\n","    (3): dw_block(\n","      (dw_block): Sequential(\n","        (0): depthwise_conv(\n","          (depthwise): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n","        )\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU()\n","      )\n","    )\n","    (4): one_by_one_block(\n","      (one_by_one_block): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU()\n","      )\n","    )\n","    (5): dw_block(\n","      (dw_block): Sequential(\n","        (0): depthwise_conv(\n","          (depthwise): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n","        )\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU()\n","      )\n","    )\n","    (6): one_by_one_block(\n","      (one_by_one_block): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU()\n","      )\n","    )\n","    (7): dw_block(\n","      (dw_block): Sequential(\n","        (0): depthwise_conv(\n","          (depthwise): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n","        )\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU()\n","      )\n","    )\n","    (8): one_by_one_block(\n","      (one_by_one_block): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU()\n","      )\n","    )\n","    (9): dw_block(\n","      (dw_block): Sequential(\n","        (0): depthwise_conv(\n","          (depthwise): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n","        )\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU()\n","      )\n","    )\n","    (10): one_by_one_block(\n","      (one_by_one_block): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU()\n","      )\n","    )\n","  )\n","  (block9): Sequential(\n","    (0): one_by_one_block(\n","      (one_by_one_block): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU()\n","      )\n","    )\n","    (1): ReLU()\n","    (2): dw_block(\n","      (dw_block): Sequential(\n","        (0): depthwise_conv(\n","          (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n","        )\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU()\n","      )\n","    )\n","  )\n","  (block10): Sequential(\n","    (0): one_by_one_block(\n","      (one_by_one_block): Sequential(\n","        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU()\n","      )\n","    )\n","    (1): dw_block(\n","      (dw_block): Sequential(\n","        (0): depthwise_conv(\n","          (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(4, 4), groups=512, bias=False)\n","        )\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU()\n","      )\n","    )\n","  )\n","  (block11): Sequential(\n","    (0): one_by_one_block(\n","      (one_by_one_block): Sequential(\n","        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU()\n","      )\n","    )\n","  )\n","  (fc_v2): Conv2d(1024, 10, kernel_size=(1, 1), stride=(1, 1), groups=2)\n",")"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"SGbH9cgt53Rl","outputId":"7ce4fbc6-53d8-4573-fd60-c477856d061d","executionInfo":{"status":"ok","timestamp":1590338604799,"user_tz":-540,"elapsed":9272,"user":{"displayName":"dy lee","photoUrl":"https://lh3.googleusercontent.com/-v5_ou76Si4Y/AAAAAAAAAAI/AAAAAAAABKQ/9DoM4jbW_BY/s64/photo.jpg","userId":"08111595787083838412"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["torchsummary.summary(net, (3, 224, 224))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 32, 112, 112]             864\n","       BatchNorm2d-2         [-1, 32, 112, 112]              64\n","              ReLU-3         [-1, 32, 112, 112]               0\n","            Conv2d-4         [-1, 32, 112, 112]             288\n","    depthwise_conv-5         [-1, 32, 112, 112]               0\n","       BatchNorm2d-6         [-1, 32, 112, 112]              64\n","              ReLU-7         [-1, 32, 112, 112]               0\n","          dw_block-8         [-1, 32, 112, 112]               0\n","              ReLU-9         [-1, 32, 112, 112]               0\n","           Conv2d-10         [-1, 64, 112, 112]           2,048\n","      BatchNorm2d-11         [-1, 64, 112, 112]             128\n","             ReLU-12         [-1, 64, 112, 112]               0\n"," one_by_one_block-13         [-1, 64, 112, 112]               0\n","           Conv2d-14           [-1, 64, 56, 56]             576\n","   depthwise_conv-15           [-1, 64, 56, 56]               0\n","      BatchNorm2d-16           [-1, 64, 56, 56]             128\n","             ReLU-17           [-1, 64, 56, 56]               0\n","         dw_block-18           [-1, 64, 56, 56]               0\n","           Conv2d-19           [-1, 64, 56, 56]           4,096\n","      BatchNorm2d-20           [-1, 64, 56, 56]             128\n","             ReLU-21           [-1, 64, 56, 56]               0\n"," one_by_one_block-22           [-1, 64, 56, 56]               0\n","           Conv2d-23           [-1, 64, 56, 56]             576\n","   depthwise_conv-24           [-1, 64, 56, 56]               0\n","      BatchNorm2d-25           [-1, 64, 56, 56]             128\n","             ReLU-26           [-1, 64, 56, 56]               0\n","         dw_block-27           [-1, 64, 56, 56]               0\n","           Conv2d-28          [-1, 128, 56, 56]           8,192\n","      BatchNorm2d-29          [-1, 128, 56, 56]             256\n","             ReLU-30          [-1, 128, 56, 56]               0\n"," one_by_one_block-31          [-1, 128, 56, 56]               0\n","             ReLU-32          [-1, 128, 56, 56]               0\n","           Conv2d-33          [-1, 128, 28, 28]           1,152\n","   depthwise_conv-34          [-1, 128, 28, 28]               0\n","      BatchNorm2d-35          [-1, 128, 28, 28]             256\n","             ReLU-36          [-1, 128, 28, 28]               0\n","         dw_block-37          [-1, 128, 28, 28]               0\n","           Conv2d-38          [-1, 128, 28, 28]          16,384\n","      BatchNorm2d-39          [-1, 128, 28, 28]             256\n","             ReLU-40          [-1, 128, 28, 28]               0\n"," one_by_one_block-41          [-1, 128, 28, 28]               0\n","           Conv2d-42          [-1, 128, 28, 28]           1,152\n","   depthwise_conv-43          [-1, 128, 28, 28]               0\n","      BatchNorm2d-44          [-1, 128, 28, 28]             256\n","             ReLU-45          [-1, 128, 28, 28]               0\n","         dw_block-46          [-1, 128, 28, 28]               0\n","           Conv2d-47          [-1, 256, 28, 28]          32,768\n","      BatchNorm2d-48          [-1, 256, 28, 28]             512\n","             ReLU-49          [-1, 256, 28, 28]               0\n"," one_by_one_block-50          [-1, 256, 28, 28]               0\n","             ReLU-51          [-1, 256, 28, 28]               0\n","           Conv2d-52          [-1, 256, 14, 14]           2,304\n","   depthwise_conv-53          [-1, 256, 14, 14]               0\n","      BatchNorm2d-54          [-1, 256, 14, 14]             512\n","             ReLU-55          [-1, 256, 14, 14]               0\n","         dw_block-56          [-1, 256, 14, 14]               0\n","           Conv2d-57          [-1, 128, 14, 14]          32,768\n","      BatchNorm2d-58          [-1, 128, 14, 14]             256\n","             ReLU-59          [-1, 128, 14, 14]               0\n"," one_by_one_block-60          [-1, 128, 14, 14]               0\n","           Conv2d-61          [-1, 128, 14, 14]           1,152\n","   depthwise_conv-62          [-1, 128, 14, 14]               0\n","      BatchNorm2d-63          [-1, 128, 14, 14]             256\n","             ReLU-64          [-1, 128, 14, 14]               0\n","         dw_block-65          [-1, 128, 14, 14]               0\n","           Conv2d-66          [-1, 128, 14, 14]          16,384\n","      BatchNorm2d-67          [-1, 128, 14, 14]             256\n","             ReLU-68          [-1, 128, 14, 14]               0\n"," one_by_one_block-69          [-1, 128, 14, 14]               0\n","           Conv2d-70          [-1, 128, 14, 14]           1,152\n","   depthwise_conv-71          [-1, 128, 14, 14]               0\n","      BatchNorm2d-72          [-1, 128, 14, 14]             256\n","             ReLU-73          [-1, 128, 14, 14]               0\n","         dw_block-74          [-1, 128, 14, 14]               0\n","           Conv2d-75          [-1, 128, 14, 14]          16,384\n","      BatchNorm2d-76          [-1, 128, 14, 14]             256\n","             ReLU-77          [-1, 128, 14, 14]               0\n"," one_by_one_block-78          [-1, 128, 14, 14]               0\n","           Conv2d-79          [-1, 128, 14, 14]           1,152\n","   depthwise_conv-80          [-1, 128, 14, 14]               0\n","      BatchNorm2d-81          [-1, 128, 14, 14]             256\n","             ReLU-82          [-1, 128, 14, 14]               0\n","         dw_block-83          [-1, 128, 14, 14]               0\n","           Conv2d-84          [-1, 128, 14, 14]          16,384\n","      BatchNorm2d-85          [-1, 128, 14, 14]             256\n","             ReLU-86          [-1, 128, 14, 14]               0\n"," one_by_one_block-87          [-1, 128, 14, 14]               0\n","           Conv2d-88          [-1, 128, 14, 14]           1,152\n","   depthwise_conv-89          [-1, 128, 14, 14]               0\n","      BatchNorm2d-90          [-1, 128, 14, 14]             256\n","             ReLU-91          [-1, 128, 14, 14]               0\n","         dw_block-92          [-1, 128, 14, 14]               0\n","           Conv2d-93          [-1, 128, 14, 14]          16,384\n","      BatchNorm2d-94          [-1, 128, 14, 14]             256\n","             ReLU-95          [-1, 128, 14, 14]               0\n"," one_by_one_block-96          [-1, 128, 14, 14]               0\n","           Conv2d-97          [-1, 128, 14, 14]           1,152\n","   depthwise_conv-98          [-1, 128, 14, 14]               0\n","      BatchNorm2d-99          [-1, 128, 14, 14]             256\n","            ReLU-100          [-1, 128, 14, 14]               0\n","        dw_block-101          [-1, 128, 14, 14]               0\n","          Conv2d-102          [-1, 256, 14, 14]          32,768\n","     BatchNorm2d-103          [-1, 256, 14, 14]             512\n","            ReLU-104          [-1, 256, 14, 14]               0\n","one_by_one_block-105          [-1, 256, 14, 14]               0\n","          Conv2d-106          [-1, 512, 14, 14]         131,072\n","     BatchNorm2d-107          [-1, 512, 14, 14]           1,024\n","            ReLU-108          [-1, 512, 14, 14]               0\n","one_by_one_block-109          [-1, 512, 14, 14]               0\n","            ReLU-110          [-1, 512, 14, 14]               0\n","          Conv2d-111            [-1, 512, 7, 7]           4,608\n","  depthwise_conv-112            [-1, 512, 7, 7]               0\n","     BatchNorm2d-113            [-1, 512, 7, 7]           1,024\n","            ReLU-114            [-1, 512, 7, 7]               0\n","        dw_block-115            [-1, 512, 7, 7]               0\n","          Conv2d-116            [-1, 512, 7, 7]         262,144\n","     BatchNorm2d-117            [-1, 512, 7, 7]           1,024\n","            ReLU-118            [-1, 512, 7, 7]               0\n","one_by_one_block-119            [-1, 512, 7, 7]               0\n","          Conv2d-120            [-1, 512, 7, 7]           4,608\n","  depthwise_conv-121            [-1, 512, 7, 7]               0\n","     BatchNorm2d-122            [-1, 512, 7, 7]           1,024\n","            ReLU-123            [-1, 512, 7, 7]               0\n","        dw_block-124            [-1, 512, 7, 7]               0\n","          Conv2d-125           [-1, 1024, 7, 7]         524,288\n","     BatchNorm2d-126           [-1, 1024, 7, 7]           2,048\n","            ReLU-127           [-1, 1024, 7, 7]               0\n","one_by_one_block-128           [-1, 1024, 7, 7]               0\n","          Conv2d-129             [-1, 10, 1, 1]           5,130\n","================================================================\n","Total params: 1,150,986\n","Trainable params: 1,150,986\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 127.29\n","Params size (MB): 4.39\n","Estimated Total Size (MB): 132.25\n","----------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TpXlEjFy53Rr","outputId":"f9d0eac4-81ae-4b0d-b1ba-37f93af7370b","executionInfo":{"status":"ok","timestamp":1590358944483,"user_tz":-540,"elapsed":13599966,"user":{"displayName":"dy lee","photoUrl":"https://lh3.googleusercontent.com/-v5_ou76Si4Y/AAAAAAAAAAI/AAAAAAAABKQ/9DoM4jbW_BY/s64/photo.jpg","userId":"08111595787083838412"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["start = time.time()  # 시작 시간 저장\n","criterion = nn.CrossEntropyLoss() #(binary 아닌)여러 클래스의 loss구하는 함수\n","optimizer = optim.SGD(net.parameters(), lr=initial_lr, momentum=0.9) ##optimizer(SGD방식): 역전파과정에서 loss function의 값을 줄여나가며 학습시킴(가중치 업데이트)\n","torch.autograd.set_detect_anomaly(True)\n","for epoch in range(100):  \n","    if epoch == 0:\n","        lr = initial_lr\n","    elif epoch % 2 == 0 and epoch != 0:\n","        lr *= 0.94\n","        optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n","    \n","    running_loss = 0.0\n","    for i, data in enumerate(train_loader, 0):\n","        inputs, labels = data  #데이터의 인풋 받아온다.s\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels) #결과와 라벨을 비교해서 loss를 구한다.\n","        loss.backward() #loss를 backward해서 기울기(미분치)를 구한다. -> 맞나?\n","        optimizer.step() #한 스텝 이동\n","        running_loss += loss.item()\n","        \n","        show_period = 250\n","        if i % show_period == show_period-1:    # print every \"show_period\" mini-batches\n","            print('[%d, %5d] loss: %.7f' %\n","                  (epoch + 1, i + 1, running_loss / show_period))\n","            running_loss = 0.0\n","       \n","    total = 0\n","    correct = 0\n","    for i, data in enumerate(valid_loader, 0):\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        outputs = net(inputs)\n","        \n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct = correct + (predicted == labels).sum().item() #맞으면 correct하고 추가함\n","        \n","    print('[%d epoch] Accuracy of the network on the validation images: %d %%' % \n","          (epoch+1, 100 * correct / total)\n","         )\n","\n","print('Finished Training')\n","print(\"time :\", time.time() - start)  # 현재시각 - 시작시간 = 실행 시간"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[1,   250] loss: 1.8506125\n","[1,   500] loss: 1.4230518\n","[1 epoch] Accuracy of the network on the validation images: 55 %\n","[2,   250] loss: 1.0747075\n","[2,   500] loss: 0.9801188\n","[2 epoch] Accuracy of the network on the validation images: 65 %\n","[3,   250] loss: 0.8303629\n","[3,   500] loss: 0.7667795\n","[3 epoch] Accuracy of the network on the validation images: 70 %\n","[4,   250] loss: 0.6778940\n","[4,   500] loss: 0.6443451\n","[4 epoch] Accuracy of the network on the validation images: 76 %\n","[5,   250] loss: 0.5910960\n","[5,   500] loss: 0.5706277\n","[5 epoch] Accuracy of the network on the validation images: 77 %\n","[6,   250] loss: 0.5244013\n","[6,   500] loss: 0.5244776\n","[6 epoch] Accuracy of the network on the validation images: 79 %\n","[7,   250] loss: 0.4674361\n","[7,   500] loss: 0.4754478\n","[7 epoch] Accuracy of the network on the validation images: 79 %\n","[8,   250] loss: 0.4498245\n","[8,   500] loss: 0.4271771\n","[8 epoch] Accuracy of the network on the validation images: 81 %\n","[9,   250] loss: 0.3948988\n","[9,   500] loss: 0.3961844\n","[9 epoch] Accuracy of the network on the validation images: 82 %\n","[10,   250] loss: 0.3689271\n","[10,   500] loss: 0.3695249\n","[10 epoch] Accuracy of the network on the validation images: 81 %\n","[11,   250] loss: 0.3421458\n","[11,   500] loss: 0.3532105\n","[11 epoch] Accuracy of the network on the validation images: 83 %\n","[12,   250] loss: 0.3269406\n","[12,   500] loss: 0.3329131\n","[12 epoch] Accuracy of the network on the validation images: 83 %\n","[13,   250] loss: 0.3004561\n","[13,   500] loss: 0.3121921\n","[13 epoch] Accuracy of the network on the validation images: 83 %\n","[14,   250] loss: 0.2905298\n","[14,   500] loss: 0.2986201\n","[14 epoch] Accuracy of the network on the validation images: 84 %\n","[15,   250] loss: 0.2686009\n","[15,   500] loss: 0.2769494\n","[15 epoch] Accuracy of the network on the validation images: 84 %\n","[16,   250] loss: 0.2493186\n","[16,   500] loss: 0.2768133\n","[16 epoch] Accuracy of the network on the validation images: 85 %\n","[17,   250] loss: 0.2317008\n","[17,   500] loss: 0.2485430\n","[17 epoch] Accuracy of the network on the validation images: 84 %\n","[18,   250] loss: 0.2287625\n","[18,   500] loss: 0.2326853\n","[18 epoch] Accuracy of the network on the validation images: 86 %\n","[19,   250] loss: 0.2122714\n","[19,   500] loss: 0.2197840\n","[19 epoch] Accuracy of the network on the validation images: 86 %\n","[20,   250] loss: 0.2136471\n","[20,   500] loss: 0.2078469\n","[20 epoch] Accuracy of the network on the validation images: 86 %\n","[21,   250] loss: 0.1808122\n","[21,   500] loss: 0.1984322\n","[21 epoch] Accuracy of the network on the validation images: 85 %\n","[22,   250] loss: 0.1816678\n","[22,   500] loss: 0.1883496\n","[22 epoch] Accuracy of the network on the validation images: 86 %\n","[23,   250] loss: 0.1640541\n","[23,   500] loss: 0.1805777\n","[23 epoch] Accuracy of the network on the validation images: 85 %\n","[24,   250] loss: 0.1631351\n","[24,   500] loss: 0.1686064\n","[24 epoch] Accuracy of the network on the validation images: 86 %\n","[25,   250] loss: 0.1542896\n","[25,   500] loss: 0.1647047\n","[25 epoch] Accuracy of the network on the validation images: 86 %\n","[26,   250] loss: 0.1518044\n","[26,   500] loss: 0.1528335\n","[26 epoch] Accuracy of the network on the validation images: 86 %\n","[27,   250] loss: 0.1323870\n","[27,   500] loss: 0.1363724\n","[27 epoch] Accuracy of the network on the validation images: 86 %\n","[28,   250] loss: 0.1437722\n","[28,   500] loss: 0.1445866\n","[28 epoch] Accuracy of the network on the validation images: 86 %\n","[29,   250] loss: 0.1214854\n","[29,   500] loss: 0.1288089\n","[29 epoch] Accuracy of the network on the validation images: 86 %\n","[30,   250] loss: 0.1178793\n","[30,   500] loss: 0.1229097\n","[30 epoch] Accuracy of the network on the validation images: 86 %\n","[31,   250] loss: 0.1076934\n","[31,   500] loss: 0.1157072\n","[31 epoch] Accuracy of the network on the validation images: 85 %\n","[32,   250] loss: 0.1022571\n","[32,   500] loss: 0.1209718\n","[32 epoch] Accuracy of the network on the validation images: 86 %\n","[33,   250] loss: 0.0971946\n","[33,   500] loss: 0.1126147\n","[33 epoch] Accuracy of the network on the validation images: 86 %\n","[34,   250] loss: 0.1025403\n","[34,   500] loss: 0.1042496\n","[34 epoch] Accuracy of the network on the validation images: 87 %\n","[35,   250] loss: 0.0851471\n","[35,   500] loss: 0.0919613\n","[35 epoch] Accuracy of the network on the validation images: 86 %\n","[36,   250] loss: 0.0885925\n","[36,   500] loss: 0.0849622\n","[36 epoch] Accuracy of the network on the validation images: 86 %\n","[37,   250] loss: 0.0689947\n","[37,   500] loss: 0.0846260\n","[37 epoch] Accuracy of the network on the validation images: 87 %\n","[38,   250] loss: 0.0756354\n","[38,   500] loss: 0.0823425\n","[38 epoch] Accuracy of the network on the validation images: 87 %\n","[39,   250] loss: 0.0745279\n","[39,   500] loss: 0.0758518\n","[39 epoch] Accuracy of the network on the validation images: 87 %\n","[40,   250] loss: 0.0750724\n","[40,   500] loss: 0.0740051\n","[40 epoch] Accuracy of the network on the validation images: 87 %\n","[41,   250] loss: 0.0608786\n","[41,   500] loss: 0.0674011\n","[41 epoch] Accuracy of the network on the validation images: 87 %\n","[42,   250] loss: 0.0639608\n","[42,   500] loss: 0.0635340\n","[42 epoch] Accuracy of the network on the validation images: 87 %\n","[43,   250] loss: 0.0605259\n","[43,   500] loss: 0.0627371\n","[43 epoch] Accuracy of the network on the validation images: 86 %\n","[44,   250] loss: 0.0638267\n","[44,   500] loss: 0.0611276\n","[44 epoch] Accuracy of the network on the validation images: 87 %\n","[45,   250] loss: 0.0518883\n","[45,   500] loss: 0.0545805\n","[45 epoch] Accuracy of the network on the validation images: 87 %\n","[46,   250] loss: 0.0543402\n","[46,   500] loss: 0.0581771\n","[46 epoch] Accuracy of the network on the validation images: 87 %\n","[47,   250] loss: 0.0433311\n","[47,   500] loss: 0.0479827\n","[47 epoch] Accuracy of the network on the validation images: 87 %\n","[48,   250] loss: 0.0457677\n","[48,   500] loss: 0.0442053\n","[48 epoch] Accuracy of the network on the validation images: 87 %\n","[49,   250] loss: 0.0426417\n","[49,   500] loss: 0.0456000\n","[49 epoch] Accuracy of the network on the validation images: 87 %\n","[50,   250] loss: 0.0388187\n","[50,   500] loss: 0.0453025\n","[50 epoch] Accuracy of the network on the validation images: 87 %\n","[51,   250] loss: 0.0363553\n","[51,   500] loss: 0.0431336\n","[51 epoch] Accuracy of the network on the validation images: 87 %\n","[52,   250] loss: 0.0376086\n","[52,   500] loss: 0.0393112\n","[52 epoch] Accuracy of the network on the validation images: 87 %\n","[53,   250] loss: 0.0313125\n","[53,   500] loss: 0.0319812\n","[53 epoch] Accuracy of the network on the validation images: 87 %\n","[54,   250] loss: 0.0334913\n","[54,   500] loss: 0.0357597\n","[54 epoch] Accuracy of the network on the validation images: 87 %\n","[55,   250] loss: 0.0349237\n","[55,   500] loss: 0.0344795\n","[55 epoch] Accuracy of the network on the validation images: 87 %\n","[56,   250] loss: 0.0356477\n","[56,   500] loss: 0.0302618\n","[56 epoch] Accuracy of the network on the validation images: 87 %\n","[57,   250] loss: 0.0276406\n","[57,   500] loss: 0.0292037\n","[57 epoch] Accuracy of the network on the validation images: 87 %\n","[58,   250] loss: 0.0315844\n","[58,   500] loss: 0.0279274\n","[58 epoch] Accuracy of the network on the validation images: 87 %\n","[59,   250] loss: 0.0269638\n","[59,   500] loss: 0.0267562\n","[59 epoch] Accuracy of the network on the validation images: 88 %\n","[60,   250] loss: 0.0278100\n","[60,   500] loss: 0.0280752\n","[60 epoch] Accuracy of the network on the validation images: 87 %\n","[61,   250] loss: 0.0241231\n","[61,   500] loss: 0.0255554\n","[61 epoch] Accuracy of the network on the validation images: 87 %\n","[62,   250] loss: 0.0245146\n","[62,   500] loss: 0.0238168\n","[62 epoch] Accuracy of the network on the validation images: 87 %\n","[63,   250] loss: 0.0238962\n","[63,   500] loss: 0.0258503\n","[63 epoch] Accuracy of the network on the validation images: 87 %\n","[64,   250] loss: 0.0237466\n","[64,   500] loss: 0.0255970\n","[64 epoch] Accuracy of the network on the validation images: 87 %\n","[65,   250] loss: 0.0217389\n","[65,   500] loss: 0.0243125\n","[65 epoch] Accuracy of the network on the validation images: 87 %\n","[66,   250] loss: 0.0217979\n","[66,   500] loss: 0.0241919\n","[66 epoch] Accuracy of the network on the validation images: 88 %\n","[67,   250] loss: 0.0197294\n","[67,   500] loss: 0.0163936\n","[67 epoch] Accuracy of the network on the validation images: 87 %\n","[68,   250] loss: 0.0208550\n","[68,   500] loss: 0.0167566\n","[68 epoch] Accuracy of the network on the validation images: 88 %\n","[69,   250] loss: 0.0172435\n","[69,   500] loss: 0.0211088\n","[69 epoch] Accuracy of the network on the validation images: 87 %\n","[70,   250] loss: 0.0194589\n","[70,   500] loss: 0.0199876\n","[70 epoch] Accuracy of the network on the validation images: 87 %\n","[71,   250] loss: 0.0159436\n","[71,   500] loss: 0.0163472\n","[71 epoch] Accuracy of the network on the validation images: 87 %\n","[72,   250] loss: 0.0197371\n","[72,   500] loss: 0.0193167\n","[72 epoch] Accuracy of the network on the validation images: 87 %\n","[73,   250] loss: 0.0194961\n","[73,   500] loss: 0.0154147\n","[73 epoch] Accuracy of the network on the validation images: 87 %\n","[74,   250] loss: 0.0152544\n","[74,   500] loss: 0.0165152\n","[74 epoch] Accuracy of the network on the validation images: 87 %\n","[75,   250] loss: 0.0159823\n","[75,   500] loss: 0.0162072\n","[75 epoch] Accuracy of the network on the validation images: 87 %\n","[76,   250] loss: 0.0154570\n","[76,   500] loss: 0.0154571\n","[76 epoch] Accuracy of the network on the validation images: 88 %\n","[77,   250] loss: 0.0141342\n","[77,   500] loss: 0.0158778\n","[77 epoch] Accuracy of the network on the validation images: 87 %\n","[78,   250] loss: 0.0148485\n","[78,   500] loss: 0.0134172\n","[78 epoch] Accuracy of the network on the validation images: 88 %\n","[79,   250] loss: 0.0135433\n","[79,   500] loss: 0.0140125\n","[79 epoch] Accuracy of the network on the validation images: 88 %\n","[80,   250] loss: 0.0122587\n","[80,   500] loss: 0.0138036\n","[80 epoch] Accuracy of the network on the validation images: 87 %\n","[81,   250] loss: 0.0140150\n","[81,   500] loss: 0.0145228\n","[81 epoch] Accuracy of the network on the validation images: 87 %\n","[82,   250] loss: 0.0136201\n","[82,   500] loss: 0.0129981\n","[82 epoch] Accuracy of the network on the validation images: 88 %\n","[83,   250] loss: 0.0133511\n","[83,   500] loss: 0.0126609\n","[83 epoch] Accuracy of the network on the validation images: 88 %\n","[84,   250] loss: 0.0127503\n","[84,   500] loss: 0.0124006\n","[84 epoch] Accuracy of the network on the validation images: 88 %\n","[85,   250] loss: 0.0132497\n","[85,   500] loss: 0.0123140\n","[85 epoch] Accuracy of the network on the validation images: 87 %\n","[86,   250] loss: 0.0104741\n","[86,   500] loss: 0.0120288\n","[86 epoch] Accuracy of the network on the validation images: 87 %\n","[87,   250] loss: 0.0143659\n","[87,   500] loss: 0.0114450\n","[87 epoch] Accuracy of the network on the validation images: 87 %\n","[88,   250] loss: 0.0113725\n","[88,   500] loss: 0.0113236\n","[88 epoch] Accuracy of the network on the validation images: 87 %\n","[89,   250] loss: 0.0113084\n","[89,   500] loss: 0.0129658\n","[89 epoch] Accuracy of the network on the validation images: 87 %\n","[90,   250] loss: 0.0113626\n","[90,   500] loss: 0.0123599\n","[90 epoch] Accuracy of the network on the validation images: 87 %\n","[91,   250] loss: 0.0110761\n","[91,   500] loss: 0.0113406\n","[91 epoch] Accuracy of the network on the validation images: 87 %\n","[92,   250] loss: 0.0111817\n","[92,   500] loss: 0.0106948\n","[92 epoch] Accuracy of the network on the validation images: 88 %\n","[93,   250] loss: 0.0104274\n","[93,   500] loss: 0.0087352\n","[93 epoch] Accuracy of the network on the validation images: 87 %\n","[94,   250] loss: 0.0110145\n","[94,   500] loss: 0.0105012\n","[94 epoch] Accuracy of the network on the validation images: 88 %\n","[95,   250] loss: 0.0094734\n","[95,   500] loss: 0.0094051\n","[95 epoch] Accuracy of the network on the validation images: 88 %\n","[96,   250] loss: 0.0100151\n","[96,   500] loss: 0.0098709\n","[96 epoch] Accuracy of the network on the validation images: 87 %\n","[97,   250] loss: 0.0097503\n","[97,   500] loss: 0.0088272\n","[97 epoch] Accuracy of the network on the validation images: 87 %\n","[98,   250] loss: 0.0081731\n","[98,   500] loss: 0.0083345\n","[98 epoch] Accuracy of the network on the validation images: 87 %\n","[99,   250] loss: 0.0088127\n","[99,   500] loss: 0.0095062\n","[99 epoch] Accuracy of the network on the validation images: 87 %\n","[100,   250] loss: 0.0087928\n","[100,   500] loss: 0.0088877\n","[100 epoch] Accuracy of the network on the validation images: 88 %\n","Finished Training\n","time : 20319.346979141235\n"],"name":"stdout"}]}]}